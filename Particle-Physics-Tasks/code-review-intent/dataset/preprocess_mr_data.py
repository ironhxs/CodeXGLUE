#!/usr/bin/env python3
"""
MR æ•°æ®é¢„å¤„ç†è„šæœ¬ - å°†çˆ¬å–çš„ GitLab MR æ•°æ®è½¬æ¢ä¸º CodeXGLUE æ ¼å¼

ç”¨é€”ï¼šä» Git_crawler1 çˆ¬å–çš„ mr_*.json æ–‡ä»¶åˆ›å»ºä»£ç å®¡æŸ¥æ„å›¾åˆ†ç±»æ•°æ®é›†
"""

import json
import os
import re
from pathlib import Path
from typing import List, Dict, Any
from collections import defaultdict
import argparse


class MRDataProcessor:
    """å¤„ç† MR æ•°æ®å¹¶è½¬æ¢ä¸ºè®­ç»ƒæ ¼å¼"""
    
    def __init__(self, mr_data_dir: str, output_dir: str):
        self.mr_data_dir = Path(mr_data_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = defaultdict(int)
        
    def extract_code_from_changes(self, changes: List[Dict]) -> str:
        """ä» MR changes ä¸­æå–ä»£ç ç‰‡æ®µ"""
        code_snippets = []
        
        for change in changes:
            new_path = change.get('new_path', '')
            diff = change.get('diff', '')
            
            # è¿‡æ»¤ï¼šåªä¿ç•™ C++, Python, ROOT macro æ–‡ä»¶
            physics_extensions = ['.cpp', '.cxx', '.cc', '.py', '.C', '.h', '.hpp', '.hh', '.cu']
            if not any(new_path.endswith(ext) for ext in physics_extensions):
                continue
            
            # æå–æ–°å¢/ä¿®æ”¹çš„ä»£ç è¡Œ
            new_lines = []
            for line in diff.split('\n'):
                if line.startswith('+') and not line.startswith('+++'):
                    new_lines.append(line[1:].strip())
            
            if new_lines:
                code_snippets.append({
                    'file': new_path,
                    'code': '\n'.join(new_lines[:50])  # é™åˆ¶é•¿åº¦
                })
        
        # åˆå¹¶æ‰€æœ‰ä»£ç ç‰‡æ®µ
        combined_code = '\n'.join([
            f"// File: {s['file']}\n{s['code']}" 
            for s in code_snippets[:3]  # æœ€å¤š3ä¸ªæ–‡ä»¶
        ])
        
        return combined_code
    
    def classify_review_intent(self, comment: str) -> int:
        """
        åˆ†ç±»å®¡æŸ¥æ„å›¾
        0 - å»ºè®®ä¼˜åŒ– (suggest optimization)
        1 - æŒ‡å‡ºé”™è¯¯ (point out bug/error)
        2 - è¯·æ±‚æ¾„æ¸… (request clarification)
        3 - æ‰¹å‡†é€šè¿‡ (approve/LGTM)
        """
        comment_lower = comment.lower()
        
        # é”™è¯¯ç›¸å…³å…³é”®è¯
        error_keywords = ['bug', 'error', 'wrong', 'incorrect', 'leak', 'crash', 
                         'segfault', 'fix', 'broken', 'fail', 'issue']
        if any(kw in comment_lower for kw in error_keywords):
            self.stats['intent_error'] += 1
            return 1
        
        # æ‰¹å‡†ç›¸å…³å…³é”®è¯
        approve_keywords = ['lgtm', 'looks good', 'approve', 'approved', 
                           'ğŸ‘', '+1', 'merge']
        if any(kw in comment_lower for kw in approve_keywords):
            self.stats['intent_approve'] += 1
            return 3
        
        # æ¾„æ¸…ç›¸å…³å…³é”®è¯
        clarify_keywords = ['why', 'how', 'clarify', 'explain', 'what', 
                           'could you', 'can you', '?']
        if any(kw in comment_lower for kw in clarify_keywords):
            self.stats['intent_clarify'] += 1
            return 2
        
        # é»˜è®¤ä¸ºä¼˜åŒ–å»ºè®®
        self.stats['intent_optimize'] += 1
        return 0
    
    def is_physics_related(self, text: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ç²’å­ç‰©ç†ç›¸å…³"""
        physics_keywords = [
            # ROOT ç›¸å…³
            'TTree', 'TBranch', 'TH1', 'TH2', 'TCanvas', 'ROOT',
            # Geant4 ç›¸å…³
            'G4', 'Geant4', 'G4Step', 'G4Track', 'G4Event',
            # ç‰©ç†æ¦‚å¿µ
            'particle', 'detector', 'energy', 'momentum', 'GeV', 'TeV',
            'Monte Carlo', 'simulation', 'reconstruction', 'trigger',
            # å¸¸è§åº“
            'CMSSW', 'Athena', 'GaudiKernel', 'ALICE', 'ATLAS', 'CMS'
        ]
        
        text_lower = text.lower()
        return any(kw.lower() in text_lower for kw in physics_keywords)
    
    def process_mr_file(self, mr_file: Path) -> List[Dict[str, Any]]:
        """å¤„ç†å•ä¸ª MR æ–‡ä»¶ï¼Œè¿”å›æ•°æ®æ ·æœ¬åˆ—è¡¨"""
        with open(mr_file, 'r', encoding='utf-8') as f:
            mr = json.load(f)
        
        samples = []
        
        # æå–ä»£ç å˜æ›´
        code_changes = self.extract_code_from_changes(mr.get('changes', []))
        
        if not code_changes:
            self.stats['skipped_no_code'] += 1
            return samples
        
        # æå– MR ä¸Šä¸‹æ–‡
        mr_context = f"Title: {mr.get('title', '')}\nDescription: {mr.get('description', '')}"
        
        # å¤„ç†æ¯æ¡éç³»ç»Ÿè¯„è®º
        comments = mr.get('comments', [])
        for comment in comments:
            if comment.get('system', False):
                continue
            
            comment_body = comment.get('body', '').strip()
            if len(comment_body) < 10:  # è¿‡æ»¤å¤ªçŸ­çš„è¯„è®º
                continue
            
            # åˆ†ç±»æ„å›¾
            intent = self.classify_review_intent(comment_body)
            
            # æ„å»ºæ ·æœ¬
            sample = {
                'idx': len(samples),
                'code': code_changes,
                'context': mr_context,
                'comment': comment_body,
                'target': intent,
                'mr_iid': mr.get('iid'),
                'mr_url': mr.get('web_url', '')
            }
            
            samples.append(sample)
            self.stats['total_samples'] += 1
        
        return samples
    
    def process_all_mrs(self) -> List[Dict[str, Any]]:
        """å¤„ç†æ‰€æœ‰ MR æ–‡ä»¶"""
        all_samples = []
        mr_files = list(self.mr_data_dir.glob('mr_*.json'))
        
        print(f"æ‰¾åˆ° {len(mr_files)} ä¸ª MR æ–‡ä»¶")
        
        for i, mr_file in enumerate(mr_files, 1):
            if i % 10 == 0:
                print(f"å¤„ç†è¿›åº¦: {i}/{len(mr_files)}")
            
            try:
                samples = self.process_mr_file(mr_file)
                all_samples.extend(samples)
            except Exception as e:
                print(f"å¤„ç† {mr_file.name} æ—¶å‡ºé”™: {e}")
                self.stats['errors'] += 1
        
        return all_samples
    
    def split_dataset(self, samples: List[Dict], train_ratio=0.8, valid_ratio=0.1):
        """åˆ’åˆ†è®­ç»ƒé›†ã€éªŒè¯é›†ã€æµ‹è¯•é›†"""
        import random
        random.seed(42)
        random.shuffle(samples)
        
        n = len(samples)
        train_end = int(n * train_ratio)
        valid_end = int(n * (train_ratio + valid_ratio))
        
        train_samples = samples[:train_end]
        valid_samples = samples[train_end:valid_end]
        test_samples = samples[valid_end:]
        
        return train_samples, valid_samples, test_samples
    
    def save_as_jsonl(self, samples: List[Dict], filename: str):
        """ä¿å­˜ä¸º JSONL æ ¼å¼"""
        filepath = self.output_dir / filename
        with open(filepath, 'w', encoding='utf-8') as f:
            for sample in samples:
                f.write(json.dumps(sample, ensure_ascii=False) + '\n')
        print(f"å·²ä¿å­˜: {filepath} ({len(samples)} æ ·æœ¬)")
    
    def print_statistics(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        print("\n" + "="*50)
        print("æ•°æ®å¤„ç†ç»Ÿè®¡:")
        print("="*50)
        for key, value in sorted(self.stats.items()):
            print(f"{key:.<30} {value}")
        print("="*50)
    
    def run(self):
        """æ‰§è¡Œå®Œæ•´çš„æ•°æ®å¤„ç†æµç¨‹"""
        print("å¼€å§‹å¤„ç† MR æ•°æ®...")
        
        # å¤„ç†æ‰€æœ‰ MR
        all_samples = self.process_all_mrs()
        
        if len(all_samples) == 0:
            print("âŒ æ²¡æœ‰ç”Ÿæˆä»»ä½•æ ·æœ¬ï¼Œè¯·æ£€æŸ¥æ•°æ®æº")
            return
        
        # åˆ’åˆ†æ•°æ®é›†
        train, valid, test = self.split_dataset(all_samples)
        
        # ä¿å­˜æ•°æ®é›†
        self.save_as_jsonl(train, 'train.jsonl')
        self.save_as_jsonl(valid, 'valid.jsonl')
        self.save_as_jsonl(test, 'test.jsonl')
        
        # æ‰“å°ç»Ÿè®¡
        self.print_statistics()
        
        # ä¿å­˜ç±»åˆ«åˆ†å¸ƒ
        self.save_label_distribution(all_samples)
    
    def save_label_distribution(self, samples: List[Dict]):
        """ä¿å­˜æ ‡ç­¾åˆ†å¸ƒç»Ÿè®¡"""
        label_dist = defaultdict(int)
        for s in samples:
            label_dist[s['target']] += 1
        
        label_names = {
            0: 'Optimization Suggestion',
            1: 'Bug/Error Report',
            2: 'Clarification Request',
            3: 'Approval/LGTM'
        }
        
        print("\næ ‡ç­¾åˆ†å¸ƒ:")
        for label, count in sorted(label_dist.items()):
            print(f"  {label} ({label_names[label]}): {count} ({count/len(samples)*100:.1f}%)")


def main():
    parser = argparse.ArgumentParser(
        description='å°† GitLab MR æ•°æ®è½¬æ¢ä¸ºä»£ç å®¡æŸ¥æ„å›¾åˆ†ç±»æ•°æ®é›†'
    )
    parser.add_argument(
        '--mr_data_dir',
        default='../../../../Git_crawler1/mr_data',
        help='MR æ•°æ®ç›®å½•ï¼ˆåŒ…å« mr_*.json æ–‡ä»¶ï¼‰'
    )
    parser.add_argument(
        '--output_dir',
        default='.',
        help='è¾“å‡ºç›®å½•'
    )
    args = parser.parse_args()
    
    # æ£€æŸ¥è¾“å…¥ç›®å½•
    if not Path(args.mr_data_dir).exists():
        print(f"âŒ MR æ•°æ®ç›®å½•ä¸å­˜åœ¨: {args.mr_data_dir}")
        print(f"æç¤ºï¼šè¯·å…ˆä½¿ç”¨ Git_crawler1 çˆ¬å– MR æ•°æ®")
        print(f"ç¤ºä¾‹ï¼šcd Git_crawler1 && python crawler.py --project-id your-project")
        return
    
    # æ‰§è¡Œå¤„ç†
    processor = MRDataProcessor(args.mr_data_dir, args.output_dir)
    processor.run()
    
    print("\nâœ… æ•°æ®å¤„ç†å®Œæˆï¼")
    print(f"ä¸‹ä¸€æ­¥ï¼šcd ../code && python run.py --do_train")


if __name__ == '__main__':
    main()
